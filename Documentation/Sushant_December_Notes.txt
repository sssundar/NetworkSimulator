6 December 2015 Things Done
---------------------------
RENO: (Keep track of RTT, varying timeout as 3x max RTT) Not Done. Known bug, no fix right now
VEGAS: Updates WS on every Ack, and updates timeout using RTT estimate.	
Both RENO and VEGAS have our 'TR' trick to partially deal with shitty stoplight links.
FAST is correct.

Played with FAST - reacts like a simple linear control system for the most part!

Logging/Measurements are now complete (debugging is another question, but
we now can generate nice plots of everything required)

Links are fixed. Train_Link gives up to 2x throughput in places, ups buffer occupancy, and decreases total time.

Usage is now 
	python visualize.py --testcase input_test1.json --tcp fast
	python visualize.py --help
It's entirely self-contained. Check out results/[rawdata|plots]

So our code is DONE. 

Next Steps:
	Presentation: lots of graphs to demonstrate it works on all test cases.
	We've never tested Test Case 2 with anything.
	Report: concise, plots, theoretical results from HW, brief architecture, brief assumptions

FAST Notes
----------

If we make FAST WS update delay 50ms, we SEE ringing - lower stability margin.
How cool is that?

Making update period too short means if you're in a bad place, you 
will update a lot. There's a base delay in the network. If you update
too much faster than that, you'll overshoot.

Making it too long means you don't correct fast enough.

We can control the strength of negative feedback using a timeout scalar.
10x is powerful. 2-3x is not.

Exponential vs. linear averaging doesn't make much of a difference at 
a packet RTT memory of 10. 20 helps a bit.

Changing alpha slows down the network but also helps us get to a steady
state.

Routing destabilizes us in Test Case 1. The sudden change in RTT 
causes oscillations. 

Funnily, 

FAST_ALPHA = 20.0 	
FAST_RTT_WINDOW_SIZE = int(20) # estimate RTTact from FAST_RTT_WINDOW_SIZE last RTTs
FAST_WS_UPDATE_TIME = 200.0 # ms ~timescale of a few packet arrivals to re-estimate RTT
FAST_TO_RTTMAX_SCALAR = 10.0 # times RTTmax as the timeout penalty
FAST_TO_ALLOWANCE = 3.0 # times RTTmax as the timeout allowance
FAST_BASE_RTTMAX = 1000.0 # ms (in case of timeout on first send)
FAST_EXPONENTIAL_DECAY = float(0.1) # 10 samples back, will see 1/e weighting

using exponential decay is unstable with routing in Test Case 1

but if we switch to linear delay averaging, it's not.

I suppose it's because exponential decay responds faster to perturbations.


Our simulator for even 2Mb on TCP FAST takes forever to run. We'll need at least
10-20 minutes per full 20Mb run so we'll need to pick our targets carefully
and run them in parallel on multiple computers.

TCP VEGAS needs a TO Allowance of 10 for Test Case 1 to work. 

